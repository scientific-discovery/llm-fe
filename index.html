<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta name="description" content="LLM-FE is an automated feature engineering framework that treats feature construction as program search, using large language models as evolutionary optimizers for tabular learning.">
  <meta name="keywords" content="Feature Engineering, Large Language Models, Tabular Learning, AutoML, Program Synthesis">
  <meta name="author" content="Nikhil Abhyankar, Parshin Shojaee, Chandan K. Reddy">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Reddy AI Lab">
  <meta property="og:title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta property="og:description" content="A framework that casts feature engineering as evolutionary program search, with LLMs generating and refining executable feature transformations.">
  <meta property="og:url" content="https://nikhilsab.github.io/LLMFE">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta name="twitter:description" content="LLM-FE uses LLMs as evolutionary optimizers to discover executable feature transformations for tabular ML.">

  <!-- Academic Metadata -->
  <meta name="citation_title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta name="citation_author" content="Abhyankar, Nikhil">
  <meta name="citation_author" content="Shojaee, Parshin">
  <meta name="citation_author" content="Reddy, Chandan K.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_pdf_url" content="https://arxiv.org/abs/2503.14434">

  <title>LLM-FE: Automated Feature Engineering with Large Language Models</title>

  <!-- Styles -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "LLM-FE: Automated Feature Engineering with Large Language Models",
    "author": [
      { "@type": "Person", "name": "Nikhil Abhyankar", "affiliation": "Virginia Tech" },
      { "@type": "Person", "name": "Parshin Shojaee", "affiliation": "Virginia Tech" },
      { "@type": "Person", "name": "Chandan K. Reddy", "affiliation": "Virginia Tech" }
    ],
    "datePublished": "2025",
    "url": "https://arxiv.org/abs/2503.14434",
    "isAccessibleForFree": true
  }
  </script>
</head>

<body>

<!-- VT Logo -->
<div class="vt-logo-container">
  <img src="static/images/vt.png" alt="Virginia Tech" class="vt-logo">
</div>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">
        LLM-FE: Automated Feature Engineering with Large Language Models
      </h1>

      <div class="is-size-5 publication-authors">
        <span class="author-block">
          <a href="https://nikhilsab.github.io/" target="_blank">Nikhil Abhyankar</a>,
        </span>
        <span class="author-block">
          <a href="https://parshinsh.github.io/" target="_blank">Parshin Shojaee</a>,
          </span>
        <span class="author-block">
          <a href="https://creddy.net/" target="_blank">Chandan K. Reddy</a>
        </span>
      </div>

      <div class="is-size-5 publication-authors">
        Virginia Tech<br>
      </div>

      <div class="publication-links">
        <span class="link-block">
          <a href="https://arxiv.org/abs/2503.14434" target="_blank"
             class="external-link button is-rounded is-dark">
            <span class="icon"><i class="fas fa-file-pdf"></i></span>
            <span>Paper</span>
          </a>
        </span>

        <span class="link-block">
          <a href="https://github.com/nikhilsab/LLMFE" target="_blank"
             class="external-link button is-rounded is-dark">
            <span class="icon"><i class="fab fa-github"></i></span>
            <span>Code</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.
      </p>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section hero">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method Overview</h2>
    <div class="content has-text-justified">
      <p>
        LLM-FE operates in a closed-loop optimization pipeline. Given dataset metadata and task
        objectives, the LLM proposes feature-transformation programs expressed as executable code.
        Each program augments the dataset with new features and is evaluated using a downstream model.
        Performance scores populate an island-based memory buffer, which is reused as in-context
        demonstrations to iteratively refine future generations.
      </p>
    </div>
  </div>
</section>

<!-- Key Features -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Features</h2>
    <div class="columns is-multiline">
      <div class="column is-half">
        <strong>Program-Based Feature Search</strong><br>
        Features are represented as executable transformation programs rather than static formulas.
      </div>
      <div class="column is-half">
        <strong>LLM-Guided Evolution</strong><br>
        Large language models act as evolutionary optimizers using performance feedback.
      </div>
      <div class="column is-half">
        <strong>Model-in-the-Loop Evaluation</strong><br>
        Feature quality is assessed via downstream learning performance.
      </div>
      <div class="column is-half">
        <strong>Interpretable Features</strong><br>
        Generated features are human-readable and reusable.
      </div>
    </div>
  </div>
</section>

<!-- Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>
    <div class="content has-text-justified">
      <p>
        Across 11 classification and 10 regression datasets, LLM-FE consistently improves performance
        over raw features and outperforms AutoFeat, OpenFE, and prior LLM-based baselines. Ablation
        studies confirm the importance of domain-aware prompting and evolutionary memory in driving
        performance gains.
      </p>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{abhyankar2025llmfe,
  title={LLM-FE: Automated Feature Engineering with Large Language Models},
  author={Abhyankar, Nikhil and Shojaee, Parshin and Reddy, Chandan K.},
  journal={arXiv preprint arXiv:2503.14434},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Built using the Academic Project Page Template.
        Licensed under Creative Commons Attribution-ShareAlike 4.0.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
