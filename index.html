<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta name="description" content="LLM-FE is an automated feature engineering framework that treats feature construction as program search, using large language models as evolutionary optimizers for tabular learning.">
  <meta name="keywords" content="Feature Engineering, Large Language Models, Tabular Learning, AutoML, Program Synthesis">
  <meta name="author" content="Nikhil Abhyankar, Parshin Shojaee, Chandan K. Reddy">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Reddy AI Lab">
  <meta property="og:title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta property="og:description" content="A framework that casts feature engineering as evolutionary program search, with LLMs generating and refining executable feature transformations.">
  <meta property="og:url" content="https://nikhilsab.github.io/LLMFE">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta name="twitter:description" content="LLM-FE uses LLMs as evolutionary optimizers to discover executable feature transformations for tabular ML.">

  <!-- Academic Metadata -->
  <meta name="citation_title" content="LLM-FE: Automated Feature Engineering with Large Language Models">
  <meta name="citation_author" content="Abhyankar, Nikhil">
  <meta name="citation_author" content="Shojaee, Parshin">
  <meta name="citation_author" content="Reddy, Chandan K.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_pdf_url" content="https://arxiv.org/abs/2503.14434">

  <title>LLM-FE: Automated Feature Engineering with Large Language Models</title>

  <!-- Styles -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "LLM-FE: Automated Feature Engineering with Large Language Models",
    "author": [
      { "@type": "Person", "name": "Nikhil Abhyankar", "affiliation": "Virginia Tech" },
      { "@type": "Person", "name": "Parshin Shojaee", "affiliation": "Virginia Tech" },
      { "@type": "Person", "name": "Chandan K. Reddy", "affiliation": "Virginia Tech" }
    ],
    "datePublished": "2025",
    "url": "https://arxiv.org/abs/2503.14434",
    "isAccessibleForFree": true
  }
  </script>
</head>

<body>

<!-- VT Logo -->
<div class="vt-logo-container">
  <img src="static/images/vt.png" alt="Virginia Tech" class="vt-logo">
</div>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">
        LLM-FE: Automated Feature Engineering with Large Language Models
      </h1>

      <div class="is-size-5 publication-authors">
        <span class="author-block">
          <a href="https://nikhilsab.github.io/" target="_blank">Nikhil Abhyankar</a>,
        </span>
        <span class="author-block">
          <a href="https://parshinsh.github.io/" target="_blank">Parshin Shojaee</a>,
          </span>
        <span class="author-block">
          <a href="https://creddy.net/" target="_blank">Chandan K. Reddy</a>
        </span>
      </div>

      <div class="is-size-5 publication-authors">
        Virginia Tech<br>
      </div>

      <div class="publication-links">
        <span class="link-block">
          <a href="https://arxiv.org/abs/2503.14434" target="_blank"
             class="external-link button is-rounded is-dark">
            <span class="icon"><i class="fas fa-file-pdf"></i></span>
            <span>Paper</span>
          </a>
        </span>

        <span class="link-block">
          <a href="https://github.com/nikhilsab/LLMFE" target="_blank"
             class="external-link button is-rounded is-dark">
            <span class="icon"><i class="fab fa-github"></i></span>
            <span>Code</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.
      </p>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section hero">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method Overview</h2>
    <div class="content has-text-justified">
      <p>
        LLM-FE casts feature engineering as a program search problem. Given dataset metadata and task objectives, a large language model generates executable feature-transformation programs using structured prompts and in-context examples. Each program is executed to augment the dataset and evaluated by training a downstream model on a validation split. High-performing programs are stored in an island-based memory buffer and reused as demonstrations to guide subsequent generations. This closed-loop process enables iterative refinement, balancing exploration and exploitation while producing interpretable, reusable features.
      </p>
      <div style="margin-top: 2rem;">
        <div class="has-text-centered">
          <img src="static/images/main_fig.jpg" alt="LLM-FE Method Overview" style="max-width: 100%; height: auto; border-radius: var(--border-radius-lg); box-shadow: var(--shadow-lg);">
        </div>
        <p style="margin-top: 1rem; font-style: italic; color: var(--text-secondary); font-size: 0.9rem; text-align: justify; line-height: 1.5; max-width: 100%;">
          Overview of the LLM-FE Framework. For a given dataset, LLM-FE follows these steps: 
          (a) New Hypothesis Generation, where an LLM generates feature transformation hypotheses as programs; 
          (b) Feature Engineering, where the program is applied to create a modified dataset; 
          (c) Model Fitting, where a prediction model is fitted and evaluated on validation data; 
          (d) Multi-Population Memory, which maintains high-scoring programs as in-context samples for iterative refinement.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Key Features -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Features</h2>
    <div class="columns is-multiline">
      <div class="column is-half">
        <strong>Program-Based Feature Search</strong><br>
        Features are represented as executable transformation programs rather than static formulas.
      </div>
      <div class="column is-half">
        <strong>LLM-Guided Evolution</strong><br>
        Large language models act as evolutionary optimizers using performance feedback.
      </div>
      <div class="column is-half">
        <strong>Model-in-the-Loop Evaluation</strong><br>
        Feature quality is assessed via downstream learning performance.
      </div>
      <div class="column is-half">
        <strong>Interpretable Features</strong><br>
        Generated features are human-readable and reusable.
      </div>
    </div>
  </div>
</section>

<!-- Quantitative Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
    <div class="content has-text-justified">
      <p>
        Across 16 classification and 10 regression datasets, LLM-FE consistently improves performance
        over raw features and outperforms AutoFeat, OpenFE, and prior LLM-based baselines.
      </p>
      
      <!-- Classification Results -->
      <div style="margin-top: 2rem;">
        <h3 class="title is-4 has-text-centered" style="margin-bottom: 1rem;">Classification Dataset Performance</h3>
        <div class="has-text-centered">
          <img src="static/images/classification_results.jpg" alt="Classification Results" style="max-width: 100%; height: auto; border-radius: var(--border-radius-lg); box-shadow: var(--shadow-lg);">
        </div>
      </div>
      
      <!-- Regression Results -->
      <div style="margin-top: 3rem;">
        <h3 class="title is-4 has-text-centered" style="margin-bottom: 1rem;">Regression Dataset Performance</h3>
        <div class="has-text-centered">
          <img src="static/images/regression_results.jpg" alt="Regression Results" style="max-width: 100%; height: auto; border-radius: var(--border-radius-lg); box-shadow: var(--shadow-lg);">
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Qualitative Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
    <div class="content has-text-justified">
      <p>
        Ablation studies confirm the importance of domain-aware prompting and evolutionary memory in driving
        performance gains.
      </p>
      
      <!-- First Rectangle at Top -->
      <div style="margin-top: 2rem;">
        <h3 class="title is-4 has-text-centered" style="margin-bottom: 1rem;">Impact of Domain Knowledge</h3>
        <div style="margin-bottom: 1rem; padding: 1rem; background-color: white; border-radius: var(--border-radius); box-shadow: var(--shadow-sm);">
          <p style="color: var(--text-secondary); font-size: 0.95rem; line-height: 1.6; margin: 0; text-align: center;">
            Integrating domain knowledge helps in generating interpretable features to improve downstream model performance.
          </p>
        </div>
        <div class="has-text-centered">
          <img src="static/images/domain.jpg" alt="Domain Analysis" style="max-width: 100%; height: auto; border-radius: var(--border-radius-lg); box-shadow: var(--shadow-lg);">
        </div>
      </div>
      
      <!-- Two Square Plots Side by Side -->
      <div class="columns" style="margin-top: 3rem;">
        <div class="column is-half">
          <h3 class="title is-4 has-text-centered" style="margin-bottom: 1rem;">Ablation Study</h3>
          <div style="margin-bottom: 1rem; padding: 1rem; background-color: white; border-radius: var(--border-radius); box-shadow: var(--shadow-sm);">
            <p style="color: var(--text-secondary); font-size: 0.95rem; line-height: 1.6; margin: 0; text-align: center;">
              Ablation study showing the contribution of each component to overall performance.
            </p>
          </div>
          <div class="has-text-centered">
            <img src="static/images/ablation.jpg" alt="Ablation Study" class="qualitative-square-img">
          </div>
        </div>
        <div class="column is-half">
          <h3 class="title is-4 has-text-centered" style="margin-bottom: 1rem;">Computational Efficiency</h3>
          <div style="margin-bottom: 1rem; padding: 1rem; background-color: white; border-radius: var(--border-radius); box-shadow: var(--shadow-sm);">
            <p style="color: var(--text-secondary); font-size: 0.95rem; line-height: 1.6; margin: 0; text-align: center;">
              LLM-FE has the best trade-off between computational cost and performance improvement.
            </p>
          </div>
          <div class="has-text-centered">
            <img src="static/images/pareto.jpg" alt="Computational Efficiency" class="qualitative-square-img">
          </div>
        </div>
      </div>
      
      <!-- Second Rectangle at Bottom -->
      <div style="margin-top: 3rem;">
        <h3 class="title is-4 has-text-centered" style="margin-bottom: 1rem;">Feature Analysis</h3>
        <div style="margin-bottom: 1rem; padding: 1rem; background-color: white; border-radius: var(--border-radius); box-shadow: var(--shadow-sm);">
          <p style="color: var(--text-secondary); font-size: 0.95rem; line-height: 1.6; margin: 0; text-align: center;">
            LLM-FE successfully generates features using simple as well as complex transformations.
          </p>
        </div>
        <div class="has-text-centered">
          <img src="static/images/features.jpg" alt="Feature Analysis" style="max-width: 100%; height: auto; border-radius: var(--border-radius-lg); box-shadow: var(--shadow-lg);">
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{abhyankar2025llmfe,
  title={LLM-FE: Automated Feature Engineering with Large Language Models},
  author={Abhyankar, Nikhil and Shojaee, Parshin and Reddy, Chandan K.},
  journal={arXiv preprint arXiv:2503.14434},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Built using the Academic Project Page Template.
        Licensed under Creative Commons Attribution-ShareAlike 4.0.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
